import os
import time
import logging
import requests
import sys
import json
import re
from datetime import datetime, timezone, timedelta
from elasticsearch import Elasticsearch, ConnectionError, TransportError
from collections import deque
from dateutil.parser import isoparse

# --- C·∫•u h√¨nh Logger ---
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logger = logging.getLogger('log-watcher')
logger.setLevel(LOG_LEVEL)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter('%(asctime)s - [LOG-WATCHER] - [%(levelname)s] - %(message)s'))
    logger.addHandler(handler)
logger.propagate = False
logging.getLogger('elasticsearch').setLevel(logging.CRITICAL)

# --- C·∫•u h√¨nh ch√≠nh (ƒê√£ ƒë∆°n gi·∫£n h√≥a) ---
ELK_URL = os.getenv("ELK_URL")
ALERT_ENDPOINT = os.getenv("ALERT_ENDPOINT", "http://alert-api:5001/alert")
CHECK_INTERVAL_SECONDS = 30
COOLDOWN_SECONDS = 900
# [ƒê∆†N GI·∫¢N H√ìA] Hardcode ƒë∆∞·ªùng d·∫´n tr·ª±c ti·∫øp. Kh√¥ng c·∫ßn bi·∫øn m√¥i tr∆∞·ªùng n·ªØa.
ARCHIVED_LOGS_DIR = "/app/archived_logs"

# --- L·∫•y KEYWORDS ƒê·ªòNG ---
DEFAULT_KEYWORDS = "error,failed,exception,denied,timeout,refused,critical,upstream timed out,connect() failed"
KEYWORDS_STR = os.getenv("LOG_WATCHER_KEYWORDS", DEFAULT_KEYWORDS)
KEYWORDS = [keyword.strip() for keyword in KEYWORDS_STR.split(',') if keyword.strip()]

# ... (C√°c h√†m get_cooldown_cache, update_cooldown_cache gi·ªØ nguy√™n) ...
def get_cooldown_cache():
    try:
        with open("/app/log_watcher_cooldown.json", 'r') as f: return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError): return {}

def update_cooldown_cache(cache):
    with open("/app/log_watcher_cooldown.json", 'w') as f: json.dump(cache, f)

def connect_to_es():
    """
    H√†m k·∫øt n·ªëi t·ªõi Elasticsearch. ƒê√£ ƒë∆∞·ª£c s·ª≠a ƒë·ªÉ x·ª≠ l√Ω l·ªói k·∫øt n·ªëi m·ªôt c√°ch an to√†n.
    """
    if not ELK_URL:
        logger.error("ELK_URL environment variable is not set.")
        return None
    try:
        client = Elasticsearch(ELK_URL, request_timeout=15, max_retries=3)
        if not client.ping():
            raise ConnectionError("Ping to Elasticsearch failed.")
        logger.info("Successfully connected to Elasticsearch.")
        return client
    except Exception as e:
        # --- ƒê√ÇY L√Ä PH·∫¶N ƒê√É S·ª¨A ---
        # D√íNG C≈® B·ªä L·ªñI:
        # logger.error(f"Error connecting to Elasticsearch: {e}")
        #
        # S·ª¨A TH√ÄNH:
        # Ghi log l·ªói k√®m theo traceback ƒë·ªÉ debug d·ªÖ d√†ng h∆°n v√† tr√°nh b·ªã crash do l·ªói IndexError.
        # ƒê√¢y l√† thay ƒë·ªïi ch√≠nh ƒë·ªÉ ch∆∞∆°ng tr√¨nh kh√¥ng b·ªã crash khi Elasticsearch kh√¥ng s·∫µn s√†ng.
        logger.error("Failed to connect to Elasticsearch. Will retry...", exc_info=True)
        return None

def process_logs(es_client, processed_log_ids):
    # ... (Ph·∫ßn ƒë·∫ßu h√†m gi·ªØ nguy√™n) ...
    start_time = datetime.now(timezone.utc) - timedelta(minutes=5)
    keyword_query_string = " OR ".join([f'"{k}"' if ' ' in k else k for k in KEYWORDS])
    query = { "bool": { "must": [ {"range": {"@timestamp": {"gte": start_time.isoformat()}}}, {"query_string": { "query": keyword_query_string, "default_field": "message", "analyze_wildcard": True }} ] } }
    logger.debug(f"Executing ES Query: {keyword_query_string}")
    try:
        res = es_client.search(index="filebeat-*", query=query, sort=[{"@timestamp": "desc"}], size=50, ignore_unavailable=True)
    except TransportError as e:
        logger.error(f"Error searching logs on Elasticsearch: {e}")
        return
    if not res['hits']['hits']:
        return

    last_alert_time = get_cooldown_cache()
    now_ts = int(time.time())

    for hit in reversed(res['hits']['hits']):
        # ... (Ph·∫ßn logic x·ª≠ l√Ω hit, cooldown gi·ªØ nguy√™n) ...
        log_id = hit['_id']
        if log_id in processed_log_ids:
            continue
        processed_log_ids.append(log_id)
        source = hit['_source']
        log_message = source.get('message', 'N/A').strip()
        hostname = source.get('host', {}).get('name') or "unknown_host"
        timestamp_str = source.get('@timestamp')
        error_signature = ' '.join(re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}|[*\d#]+|[:.]', '', log_message).split()[:7])
        cache_key = f"{hostname}:{error_signature}"
        if (now_ts - last_alert_time.get(cache_key, 0)) < COOLDOWN_SECONDS:
            continue
        last_alert_time[cache_key] = now_ts
        update_cooldown_cache(last_alert_time)

        incident_id = f"LOG_{datetime.now().strftime('%H%M%S')}_{hostname.replace('.', '_')}"
        logger.warning(f"‚úÖ DETECTED new error log (Incident: {incident_id}) from '{hostname}': {log_message[:150]}...")

        # [ƒê∆†N GI·∫¢N H√ìA] L∆∞u log g·ªëc v√†o file, kh√¥ng c·∫ßn c·∫•u h√¨nh g√¨ th√™m
        try:
            today_str = datetime.now().strftime('%Y-%m-%d')
            daily_archive_path = os.path.join(ARCHIVED_LOGS_DIR, today_str)
            os.makedirs(daily_archive_path, exist_ok=True)

            log_filepath = os.path.join(daily_archive_path, f"{incident_id}.log")
            with open(log_filepath, 'w', encoding='utf-8') as f:
                f.write(log_message)
            logger.info(f"   -> Archived log to {log_filepath}")
        except Exception as e:
            logger.error(f"   -> Failed to archive log for incident '{incident_id}': {e}")

        unix_timestamp = int(isoparse(timestamp_str).timestamp()) if timestamp_str else now_ts
        payload = { "source": "LogWatcher", "metric": "log_error_detected", "instance": hostname, "severity": "warning", "trigger_log": log_message, "timestamp": unix_timestamp }

        try:
            requests.post(ALERT_ENDPOINT, json=payload, timeout=60)
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to send alert for incident '{incident_id}': {e}")


if __name__ == "__main__":
    es_client = None
    processed_log_ids = deque(maxlen=5000)
    logger.info(f"üöÄ Log Watcher (v9.5 - Simplified) is starting...")
    logger.info(f"Watching for keywords: {KEYWORDS}")
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c ch√≠nh t·ªìn t·∫°i khi kh·ªüi ƒë·ªông
    os.makedirs(ARCHIVED_LOGS_DIR, exist_ok=True)
    logger.info(f"Archiving detected logs to: {ARCHIVED_LOGS_DIR}")

    while True:
        try:
            if es_client is None or not es_client.ping():
                # Th√¥ng b√°o r·∫±ng ƒëang c·ªë k·∫øt n·ªëi l·∫°i
                if es_client is not None:
                    logger.warning("Elasticsearch connection lost. Attempting to reconnect...")
                es_client = connect_to_es()
            
            if es_client:
                process_logs(es_client, processed_log_ids)
            else:
                # N·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c, ƒë·ª£i m·ªôt kho·∫£ng th·ªùi gian tr∆∞·ªõc khi th·ª≠ l·∫°i
                time.sleep(30)
                continue # B·ªè qua v√≤ng l·∫∑p hi·ªán t·∫°i v√† th·ª≠ k·∫øt n·ªëi l·∫°i ·ªü v√≤ng l·∫∑p sau
        except Exception as e:
            logger.critical(f"Critical error in main loop: {e}", exc_info=True)
            es_client = None # ƒê·∫∑t l·∫°i client ƒë·ªÉ v√≤ng l·∫∑p sau k·∫øt n·ªëi l·∫°i
        
        time.sleep(CHECK_INTERVAL_SECONDS)
