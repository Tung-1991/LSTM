# FILE: flaskAPI/alert.py
# VERSION: 2.4 - YAML-Immutable & Intelligent Execution
import os
import json
import logging
import re
import requests
import yaml
import sys
from flask import Flask, request, jsonify
from datetime import datetime
from functools import lru_cache
from waitress import serve

# Gi·∫£ l·∫≠p h√†m elk n·∫øu kh√¥ng t·ªìn t·∫°i ƒë·ªÉ tr√°nh l·ªói
try:
    from elk import get_logs_for_instance
except ImportError:
    def get_logs_for_instance(*args, **kwargs):
        logging.warning("elk.py not found or failed to import, returning empty log string.")
        return "Could not retrieve logs from ELK."

# --- C·∫•u h√¨nh ---
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
RAG_API_URL = os.getenv("RAG_API_URL", "http://172.27.119.158:5005/ask")
SRE_AGENT_URL = os.getenv("SRE_AGENT_URL", "http://sre-agent:5002/execute")
ALERTMANAGER_URL = os.getenv("ALERTMANAGER_URL", "http://192.168.111.111:9093/api/v1/alerts")
KNOWLEDGE_SOURCE_DIR = os.getenv("KNOWLEDGE_SOURCE_DIR", "/rag_source")
OUTPUT_DIR = "/app/ai_suggestions"
VERIFY_SSL = os.getenv("VERIFY_SSL", "true").lower() in ('true', '1', 't')
# [M·ªöI v2.4] C√°c t·ª´ kh√≥a trong t√™n playbook ƒë·ªÉ "d·ª´ng th√¥ng minh"
STOP_ON_SUCCESS_KEYWORDS = ['restart', 'fix', 'reboot', 'start', 'enable', 'remedy']

# --- Logger ---
logger = logging.getLogger('alert-api')
logger.setLevel(LOG_LEVEL)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - [ALERT-API] - [%(levelname)s] - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.propagate = False
logging.getLogger('urllib3').setLevel(logging.WARNING)

app = Flask(__name__)
os.makedirs(OUTPUT_DIR, exist_ok=True)

@lru_cache(maxsize=128)
def find_knowledge_by_host(host_identifier: str):
    logger.info(f"[KNOWLEDGE_SEARCH] Searching for knowledge related to '{host_identifier}' in '{KNOWLEDGE_SOURCE_DIR}'")
    if not os.path.exists(KNOWLEDGE_SOURCE_DIR):
        logger.warning(f"[KNOWLEDGE_SEARCH] Directory '{KNOWLEDGE_SOURCE_DIR}' does not exist.")
        return None, None, None

    search_keys = [host_identifier]
    is_ip = re.match(r'^\d{1,3}(?:\.\d{1,3}){3}$', host_identifier) is not None
    if not is_ip:
        hostname_part = host_identifier.split('.')[0]
        if hostname_part != host_identifier:
            search_keys.append(hostname_part)
    
    for key in search_keys:
        for root, _, files in os.walk(KNOWLEDGE_SOURCE_DIR):
            for filename in files:
                if filename.endswith((".yml", ".yaml")):
                    filepath = os.path.join(root, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            doc = yaml.safe_load(f)
                        hosts_in_doc = doc.get('metadata', {}).get('hosts', [])
                        if not isinstance(hosts_in_doc, list): continue

                        for host_info in hosts_in_doc:
                            if isinstance(host_info, dict) and key in (host_info.get('hostname'), host_info.get('ip')):
                                logger.info(f"[KNOWLEDGE_SEARCH] >> MATCH FOUND << File: '{filepath}', Host: {host_info}")
                                return doc, host_info, filepath
                    except yaml.YAMLError as ye:
                        logger.error(f"YAML syntax error in {filepath}: {ye}")
                    except Exception as e:
                        logger.error(f"Error processing knowledge file {filepath}: {e}", exc_info=True)

    logger.warning(f"[KNOWLEDGE_SEARCH] >> NO MATCH << for '{host_identifier}'. Proceeding with AI Fallback.")
    return None, None, None

def call_llm_for_analysis(prompt: str, incident_id: str) -> str:
    if not RAG_API_URL:
        logger.error(f"[{incident_id}] RAG_API_URL is not configured. AI analysis is disabled.")
        return "AI analysis is unavailable due to configuration error."
    try:
        response = requests.post(RAG_API_URL, json={"message": prompt}, timeout=120, verify=VERIFY_SSL)
        response.raise_for_status()
        reply = response.json().get("reply", "No reply content from AI.")
        logger.info(f"[{incident_id}] Received AI analysis successfully.")
        return reply.strip()
    except requests.RequestException as e:
        logger.error(f"[{incident_id}] Failed to call RAG API at {RAG_API_URL}: {e}")
        return f"AI analysis failed due to connection error: {e}"

def dispatch_sre_action(playbook: dict, target_ip: str, target_hostname: str, incident_id: str):
    if not SRE_AGENT_URL:
        logger.warning(f"[{incident_id}] SRE_AGENT_URL is not configured. Skipping automation.")
        return {"status": "SKIPPED", "output": "SRE_AGENT_URL not configured."}

    command_template = playbook.get("target")
    if not command_template or not isinstance(command_template, str):
        logger.error(f"[{incident_id}] Invalid or missing 'target' in playbook: {playbook.get('name')}")
        return {"status": "ERROR", "output": "Invalid playbook target."}
    
    final_command = command_template.replace("{{TARGET_HOST}}", str(target_hostname)).replace("{{TARGET_IP}}", str(target_ip))

    logger.warning(f"[{incident_id}] DISPATCHING AUTOMATION: '{playbook.get('name')}' on host '{target_hostname}'")
    logger.info(f"[{incident_id}] Final command: '{final_command}'")

    payload = {"command": final_command, "target_host": target_hostname}
    try:
        response = requests.post(SRE_AGENT_URL, json=payload, timeout=180, verify=VERIFY_SSL)
        response.raise_for_status()
        logger.info(f"[{incident_id}] SRE Agent responded for action '{playbook.get('name')}'")
        return response.json()
    except requests.RequestException as e:
        logger.error(f"[{incident_id}] Failed to dispatch command to SRE Agent: {e}")
        error_output = str(e)
        if getattr(e, 'response', None) is not None:
            try:
                error_output = e.response.json().get('output', e.response.text)
            except json.JSONDecodeError:
                error_output = e.response.text
        else: 
            error_output = f"Agent unreachable or request timed out: {str(e)}"
        
        return {"status": "ERROR", "output": f"Failed to contact SRE Agent: {error_output}"}

# ========================================================================
# [N√ÇNG C·∫§P v2.4] H√†m process_incident
# ========================================================================
def process_incident(data: dict):
    raw_instance = data.get("instance", "unknown_instance")
    instance_identifier = raw_instance.split(":")[0]
    incident_id = f"{datetime.now().strftime('%y%m%d_%H%M%S')}_{instance_identifier.replace('.', '_')}"
    logger.info(f"[{incident_id}] === New Incident Received for '{instance_identifier}' ===")

    # --- B∆∞·ªõc 1: L·∫•y ng·ªØ c·∫£nh ban ƒë·∫ßu (s·ª± c·ªë & tri th·ª©c) ---
    incident_context = f"S·ª± c·ªë ban ƒë·∫ßu: {json.dumps(data)}\n"
    if data.get("source") == "LogWatcher":
        incident_context += f"Log g√¢y ra s·ª± c·ªë: {data.get('trigger_log', 'N/A')}\n"
    
    related_logs = get_logs_for_instance(instance_identifier, around_time=datetime.fromtimestamp(data.get('timestamp', datetime.now().timestamp())), window_minutes=5)
    if related_logs:
        log_limit = 8000
        if len(related_logs) > log_limit: 
            related_logs = related_logs[:log_limit] + "\n... (logs truncated)"
            logger.warning(f"[{incident_id}] Related logs were truncated to {log_limit} characters.")
        incident_context += f"C√°c log li√™n quan kh√°c:\n{related_logs}"

    knowledge_doc, matched_host_info, knowledge_filepath = find_knowledge_by_host(instance_identifier)

    # --- B∆∞·ªõc 2: Th·ª±c thi chu·ªói playbook t·ª± ƒë·ªông (TR∆Ø·ªöC KHI G·ªåI AI) ---
    all_execution_results = []
    action_plan = {}
    host_alias_for_ansible = instance_identifier
    
    if knowledge_doc and matched_host_info:
        host_alias_for_ansible = matched_host_info.get('hostname', instance_identifier)
        action_plan = knowledge_doc.get('actionable_plan', {})
        remediation_playbooks = action_plan.get('remediation_playbooks', [])
        
        if isinstance(remediation_playbooks, list):
            for playbook in remediation_playbooks:
                if playbook.get("allow_automation") is True:
                    playbook_name = playbook.get('name', '').lower()
                    logger.info(f"[{incident_id}] Found automation-allowed playbook: '{playbook_name}'")
                    
                    execution_result = dispatch_sre_action(
                        playbook,
                        target_ip=matched_host_info.get('ip', instance_identifier),
                        target_hostname=host_alias_for_ansible,
                        incident_id=incident_id
                    )
                    
                    all_execution_results.append({ "name": playbook.get('name'), "result": execution_result })

                    # [LOGIC D·ª™NG TH√îNG MINH v2.4]
                    if execution_result.get("status") == "SUCCESS" and any(keyword in playbook_name for keyword in STOP_ON_SUCCESS_KEYWORDS):
                        logger.warning(f"[{incident_id}] Remediation playbook '{playbook_name}' succeeded. Halting automation sequence as a precaution.")
                        break
    
    # --- B∆∞·ªõc 3: T·ªïng h·ª£p ng·ªØ c·∫£nh cho AI ---
    automation_context = "Kh√¥ng c√≥ h√†nh ƒë·ªông t·ª± ƒë·ªông n√†o ƒë∆∞·ª£c th·ª±c hi·ªán."
    if all_execution_results:
        automation_context = "C√°c h√†nh ƒë·ªông t·ª± ƒë·ªông sau ƒë√£ ƒë∆∞·ª£c th·ª±c thi:\n"
        for res in all_execution_results:
            status = res['result'].get('status', 'N/A')
            output_line = res['result'].get('output', '').strip().splitlines()[-1] if res['result'].get('output') else "N/A"
            automation_context += f"- Playbook '{res['name']}': Tr·∫°ng th√°i = {status}. K·∫øt qu·∫£ t√≥m t·∫Øt: {output_line[:150]}\n"

    # --- B∆∞·ªõc 4: G·ªçi AI ƒë·ªÉ c√≥ Ph√¢n t√≠ch Cu·ªëi c√πng ---
    knowledge_content = "Kh√¥ng c√≥ tri th·ª©c c·ª• th·ªÉ."
    knowledge_source = "AI Fallback"
    if knowledge_doc:
        knowledge_content = knowledge_doc.get('content', 'Kh√¥ng c√≥ n·ªôi dung m√¥ t·∫£.')
        if knowledge_filepath:
            knowledge_source = os.path.basename(knowledge_filepath)

    prompt = f"""B·∫°n l√† m·ªôt k·ªπ s∆∞ SRE chuy√™n gia. M·ªôt s·ª± c·ªë ƒë√£ x·∫£y ra. H·ªá th·ªëng ƒë√£ t·ª± ƒë·ªông th·ª±c hi·ªán m·ªôt s·ªë h√†nh ƒë·ªông.
H√£y ph√¢n t√≠ch t·∫•t c·∫£ th√¥ng tin d∆∞·ªõi ƒë√¢y v√† ƒë∆∞a ra m·ªôt b·∫£n ch·∫©n ƒëo√°n cu·ªëi c√πng s√∫c t√≠ch, chuy√™n nghi·ªáp.

**1. D·ªÆ LI·ªÜU S·ª∞ C·ªê BAN ƒê·∫¶U (Host: {instance_identifier}):**
---
{incident_context}
---
**2. TRI TH·ª®C H·ªÜ TH·ªêNG (T·ª´ file {knowledge_source}):**
---
{knowledge_content}
---
**3. K·∫æT QU·∫¢ T·ª∞ ƒê·ªòNG H√ìA ƒê√É TH·ª∞C THI:**
---
{automation_context}
---
**Y√äU C·∫¶U:** D·ª±a v√†o c·∫£ 3 ngu·ªìn th√¥ng tin tr√™n, h√£y tr·∫£ l·ªùi:
- **Ch·∫©n ƒëo√°n:** (1-2 c√¢u) Nguy√™n nh√¢n g·ªëc r·ªÖ c√≥ th·ªÉ l√† g√¨? S·ª± c·ªë ƒë√£ ƒë∆∞·ª£c kh·∫Øc ph·ª•c ch∆∞a?
- **H√†nh ƒë·ªông ti·∫øp theo:** (N·∫øu c·∫ßn) ƒê·ªÅ xu·∫•t c√°c b∆∞·ªõc ki·ªÉm tra ho·∫∑c x·ª≠ l√Ω th·ªß c√¥ng ti·∫øp theo.
"""
    analysis_summary = call_llm_for_analysis(prompt, incident_id)

    # --- B∆∞·ªõc 5: T·∫°o v√† G·ª≠i Alert ---
    system_code = knowledge_doc.get('metadata', {}).get('system_code', 'General') if knowledge_doc else "General"
    title = knowledge_doc.get('metadata', {}).get('title', f"AI Analysis for {instance_identifier}") if knowledge_doc else f"AI Analysis for {instance_identifier}"

    final_alert = format_final_alert(
        incident_id, data, instance_identifier, host_alias_for_ansible, analysis_summary,
        action_plan, all_execution_results, knowledge_source, 
        title, system_code, data.get('trigger_log')
    )
    
    send_to_alertmanager(final_alert, incident_id)
    try:
        with open(os.path.join(OUTPUT_DIR, f"{incident_id}.md"), "w", encoding='utf-8') as f:
            f.write(f"# {final_alert['annotations']['summary']}\n\n{final_alert['annotations']['description']}")
    except Exception as e:
        logger.error(f"[{incident_id}] Could not write debug markdown file: {e}")

    return {"status": "processed", "incident_id": incident_id, "knowledge_source": knowledge_source}

# ========================================================================
# [N√ÇNG C·∫§P v2.4] H√†m format_final_alert
# ========================================================================
def format_final_alert(incident_id, data, instance, host_alias, analysis, plan, all_exec_results, source, title, system_code, trigger_log):
    severity_upper = data.get('severity', 'warning').upper()
    safe_title = title.strip()[:200]
    safe_host_alias = host_alias[:120]
    summary = f"[{severity_upper}] {system_code}: {safe_title} on {safe_host_alias}"

    description_parts = ["### üìä T√≥m t·∫Øt s·ª± c·ªë"]
    description_parts.append(f"- **Host:** `{instance}` (Alias: `{host_alias}`)")
    if trigger_log: 
        description_parts.append(f"- **Log k√≠ch ho·∫°t:** `{trigger_log}`")
    else: 
        description_parts.append(f"- **Metric:** `{data.get('metric', 'N/A')}` | **Value:** `{data.get('value', 'N/A')}`")

    # [M·ªöI v2.4] Ph·∫ßn 2: H√†nh ƒë·ªông t·ª± ƒë·ªông (Hi·ªÉn th·ªã tr∆∞·ªõc ƒë·ªÉ ng∆∞·ªùi v·∫≠n h√†nh th·∫•y ngay)
    if all_exec_results:
        description_parts.append("\n### ‚ö° H√†nh ƒë·ªông T·ª± ƒë·ªông ƒë√£ th·ª±c thi")
        for res_item in all_exec_results:
            name = res_item.get("name")
            result = res_item.get("result", {})
            status = result.get("status", "UNKNOWN")
            output = result.get("output", "No output from agent.")

            description_parts.append(f"\n- **Playbook:** `{name}`")
            if status == "SUCCESS":
                recap_line = re.search(r"PLAY RECAP.*?\n(.*?)\n", output, re.DOTALL)
                summary_output = recap_line.group(1).strip() if recap_line else "Completed successfully."
                description_parts.append(f"  - **Tr·∫°ng th√°i:** ‚úÖ **TH√ÄNH C√îNG**")
                description_parts.append(f"  - **K·∫øt qu·∫£:** `{summary_output}`")
            else:
                error_summary = output.splitlines()[-1] if output else "Agent returned an error."
                description_parts.append(f"  - **Tr·∫°ng th√°i:** ‚ùå **TH·∫§T B·∫†I / L·ªñI** (`{status}`)")
                description_parts.append(f"  - **Chi ti·∫øt l·ªói:** `{error_summary}`")
    
    # Ph·∫ßn 3: Ch·∫©n ƒëo√°n c·ªßa AI (Sau khi ƒë√£ c√≥ m·ªçi th√¥ng tin)
    description_parts.append("\n### ü§ñ Ch·∫©n ƒëo√°n cu·ªëi c√πng t·ª´ AI SRE")
    description_parts.append(f"_{analysis}_")
    description_parts.append(f"> Ngu·ªìn tri th·ª©c: `{source}`")

    # Ph·∫ßn 4: G·ª£i √Ω c√°c b∆∞·ªõc th·ªß c√¥ng
    manual_steps = (plan.get('investigation_steps', []) or []) + [p for p in (plan.get('remediation_playbooks', []) or []) if p.get('allow_automation') is not True]
    if manual_steps:
        description_parts.append("\n### üõ†Ô∏è G·ª£i √Ω c√°c b∆∞·ªõc x·ª≠ l√Ω (Th·ªß c√¥ng)")
        for s in manual_steps:
            command = s.get('command') or s.get('target', 'N/A').replace('{{TARGET_HOST}}', host_alias)
            description_parts.append(f"- **{s.get('name')}**: `{command}`")

    # T·∫°o labels cu·ªëi c√πng
    labels = { "alertname": f"AIOps_{system_code}", "instance": instance, "severity": data.get('severity', 'warning'), "source": "AI_SRE", "host_alias": host_alias }
    if all_exec_results:
        labels["automation_attempted"] = "true"
        # Tr·∫°ng th√°i t·ªïng h·ª£p: n·∫øu d√π ch·ªâ 1 playbook th·∫•t b·∫°i th√¨ coi l√† failed.
        final_status = "SUCCESS" if all(res['result'].get('status') == 'SUCCESS' for res in all_exec_results) else "FAILED"
        labels["automation_status"] = final_status.lower()

    return {
        "labels": labels,
        "annotations": { "summary": summary, "description": "\n".join(description_parts) }
    }

def send_to_alertmanager(payload, incident_id):
    if not ALERTMANAGER_URL:
        logger.info(f"[{incident_id}] ALERTMANAGER_URL not set. Skipping send.")
        return
    try:
        res = requests.post(ALERTMANAGER_URL, json=[payload], timeout=15, verify=VERIFY_SSL)
        res.raise_for_status()
        logger.info(f"[{incident_id}] Successfully sent alert to Alertmanager.")
    except requests.RequestException as e:
        logger.error(f"[{incident_id}] Failed to send alert to Alertmanager: {e}")

@app.route("/alert", methods=["POST"])
def alert_endpoint():
    data = request.get_json()
    if not data or "instance" not in data:
        return jsonify({"status": "error", "message": "Invalid JSON or missing 'instance'"}), 400
    
    result = process_incident(data)
    return jsonify(result)

@app.route("/reload", methods=["POST"])
def reload_cache():
    """Endpoint to clear the LRU cache for knowledge files."""
    try:
        cleared_count = find_knowledge_by_host.cache_info().currsize
        find_knowledge_by_host.cache_clear()
        logger.warning(f"Knowledge cache cleared successfully. ({cleared_count} items removed)")
        return jsonify({"status": "success", "message": "Cache cleared."}), 200
    except Exception as e:
        logger.error(f"Failed to clear cache: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

if __name__ == '__main__':
    # S·ª≠ d·ª•ng Waitress thay v√¨ Gunicorn v·ªõi nhi·ªÅu worker ƒë·ªÉ ƒë·∫£m b·∫£o lru_cache
    # ho·∫°t ƒë·ªông nh·∫•t qu√°n. Endpoint /reload s·∫Ω x√≥a cache tr√™n ti·∫øn tr√¨nh duy nh·∫•t n√†y.
    logger.info("üöÄ AI SRE Alerting Service (v2.4 - YAML-Immutable & Intelligent Execution) is ready.")
    logger.info(f"Knowledge Source Directory: {KNOWLEDGE_SOURCE_DIR}")
    logger.info(f"SSL Verification for outgoing requests: {VERIFY_SSL}")
    if not RAG_API_URL:
        logger.error("FATAL: RAG_API_URL environment variable is not set!")
    serve(app, host='0.0.0.0', port=5001, threads=10)
