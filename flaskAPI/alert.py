# FILE: flaskAPI/alert.py
# VERSION: 9.4 - FINAL - S·ª≠a l·ªói NameError v√† Logic Th·∫©m ƒë·ªãnh
# T√°c gi·∫£: ƒê·ªëi t√°c l·∫≠p tr√¨nh AI (Thi h√†nh theo logic ƒë√£ th·ªëng nh·∫•t)

import os
import json
import logging
import re
import requests
import yaml
import sys
from flask import Flask, request, jsonify
from datetime import datetime
from waitress import serve
from functools import lru_cache

# --- Import c√°c module t√πy ch·ªânh ---
import memory

try:
    from elk import get_logs_for_instance
except ImportError:
    # H√†m d·ª± ph√≤ng n·∫øu elk.py kh√¥ng t·ªìn t·∫°i
    def get_logs_for_instance(*args, **kwargs):
        logging.warning("elk.py not found or failed to import, returning empty log string.")
        return "Could not retrieve logs from ELK."

# --- C·∫•u h√¨nh ---
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
RAG_API_URL = os.getenv("RAG_API_URL")
SRE_AGENT_URL = os.getenv("SRE_AGENT_URL")
ALERTMANAGER_URL = os.getenv("ALERTMANAGER_URL")
KNOWLEDGE_SOURCE_DIR = os.getenv("KNOWLEDGE_SOURCE_DIR", "/rag_source")
OUTPUT_DIR = "/app/ai_suggestions"
STOP_ON_SUCCESS_KEYWORDS = ['restart', 'reboot', 'fix', 'remedy', 'reload', 'start', 'clear', 'delete']

# --- Logger ---
logger = logging.getLogger('alert-api')
logger.setLevel(LOG_LEVEL)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter('%(asctime)s - [ALERT-API] - [%(levelname)s] - %(message)s'))
    logger.addHandler(handler)
logging.getLogger('urllib3').setLevel(logging.WARNING)

# --- Kh·ªüi t·∫°o ·ª©ng d·ª•ng Flask ---
app = Flask(__name__)

@lru_cache(maxsize=512)
def load_yaml_from_file(path: str):
    """ƒê·ªçc v√† cache n·ªôi dung file YAML ƒë·ªÉ tr√°nh ƒë·ªçc l·∫°i t·ª´ ƒëƒ©a."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.error(f"Failed to load or parse YAML from {path}: {e}")
        return None

def call_llm(messages: list, incident_id: str) -> str:
    """H√†m g·ªçi LLM chung, c√≥ th·ªÉ d√πng cho c·∫£ th·∫©m ƒë·ªãnh v√† ph√¢n t√≠ch."""
    if not RAG_API_URL:
        logger.error(f"[{incident_id}] RAG_API_URL is not configured.")
        return "LLM_UNAVAILABLE"
    try:
        payload = {"message": messages[0]['content']}
        timeout = 30 if "Ch·ªâ tr·∫£ l·ªùi 'C√ì' ho·∫∑c 'KH√îNG'" in messages[0]['content'] else 120
        response = requests.post(RAG_API_URL, json=payload, timeout=timeout)
        response.raise_for_status()
        return response.json().get("reply", "").strip()
    except requests.RequestException as e:
        logger.error(f"[{incident_id}] Failed to call LLM API at {RAG_API_URL}: {e}")
        return "LLM_ERROR"

# === [S·ª¨A L·ªñI NAMEERROR] ƒê·ªäNH NGHƒ®A ƒê·∫¶Y ƒê·ª¶ C√ÅC H√ÄM B·ªä THI·∫æU ===

def dispatch_sre_action(playbook: dict, target_ip: str, target_hostname: str, incident_id: str):
    """G·ª≠i y√™u c·∫ßu th·ª±c thi playbook ƒë·∫øn SRE Agent."""
    if not SRE_AGENT_URL:
        logger.warning(f"[{incident_id}] SRE_AGENT_URL is not configured. Skipping automation.")
        return {"status": "SKIPPED", "output": "SRE_AGENT_URL not configured."}
    
    command_template = playbook.get("target")
    if not command_template or not isinstance(command_template, str):
        logger.error(f"[{incident_id}] Invalid or missing 'target' in playbook: {playbook.get('name')}")
        return {"status": "ERROR", "output": "Invalid playbook target."}
    
    final_command = command_template.replace("{{TARGET_HOST}}", str(target_hostname)).replace("{{TARGET_IP}}", str(target_ip))
    logger.warning(f"[{incident_id}] DISPATCHING AUTOMATION: '{playbook.get('name')}' on host '{target_hostname}'")
    logger.info(f"[{incident_id}] Final command: '{final_command}'")
    payload = {"command": final_command, "target_host": target_hostname}
    
    try:
        response = requests.post(SRE_AGENT_URL, json=payload, timeout=180)
        response.raise_for_status()
        logger.info(f"[{incident_id}] SRE Agent responded for action '{playbook.get('name')}'")
        return response.json()
    except requests.RequestException as e:
        logger.error(f"[{incident_id}] Failed to dispatch command to SRE Agent: {e}")
        error_output = str(e)
        if getattr(e, 'response', None) is not None:
            try:
                error_output = e.response.json().get('output', e.response.text)
            except json.JSONDecodeError:
                error_output = e.response.text
        else:
            error_output = f"Agent unreachable or request timed out: {str(e)}"
        return {"status": "ERROR", "output": f"Failed to contact SRE Agent: {error_output}"}

def format_final_alert(incident_id, data, instance, host_alias, analysis, plan, all_exec_results, source, title, system_code, trigger_log):
    """ƒê·ªãnh d·∫°ng c·∫£nh b√°o cu·ªëi c√πng ƒë·ªÉ g·ª≠i ƒëi."""
    severity_upper = data.get('severity', 'warning').upper()
    safe_title = title.strip()[:200]
    safe_host_alias = host_alias[:120]
    summary = f"[{severity_upper}] {system_code}: {safe_title} on {safe_host_alias}"
    
    description_parts = [f"### üìä T√≥m t·∫Øt s·ª± c·ªë", f"> Ngu·ªìn tri th·ª©c: `{source}`", f"- **Host:** `{instance}` (Alias: `{host_alias}`)"]
    if trigger_log:
        description_parts.append(f"- **Log k√≠ch ho·∫°t:** `{trigger_log}`")
    else:
        description_parts.append(f"- **Metric:** `{data.get('metric', 'N/A')}` | **Value:** `{data.get('value', 'N/A')}`")

    if all_exec_results:
        description_parts.append("\n### ‚ö° H√†nh ƒë·ªông T·ª± ƒë·ªông ƒë√£ th·ª±c thi")
        for res_item in all_exec_results:
            name = res_item.get("name")
            result = res_item.get("result", {})
            status = result.get("status", "UNKNOWN")
            output = result.get("output", "No output from agent.")
            description_parts.append(f"\n- **Playbook:** `{name}`")
            if status == "SUCCESS":
                recap_line = re.search(r"PLAY RECAP.*?\n(.*?)\n", output, re.DOTALL)
                summary_output = recap_line.group(1).strip() if recap_line else "Completed successfully."
                description_parts.append(f"  - **Tr·∫°ng th√°i:** ‚úÖ **TH√ÄNH C√îNG**")
                description_parts.append(f"  - **K·∫øt qu·∫£:** `{summary_output}`")
            else:
                error_summary = output.splitlines()[-1] if output else "Agent returned an error."
                description_parts.append(f"  - **Tr·∫°ng th√°i:** ‚ùå **TH·∫§T B·∫†I / L·ªñI** (`{status}`)")
                description_parts.append(f"  - **Chi ti·∫øt l·ªói:** `{error_summary}`")

    description_parts.append("\n### ü§ñ Ch·∫©n ƒëo√°n cu·ªëi c√πng t·ª´ AI SRE")
    description_parts.append(f"_{analysis}_")

    manual_steps = (plan.get('investigation_steps', []) or []) + [p for p in (plan.get('remediation_playbooks', []) or []) if not p.get('allow_automation')]
    if manual_steps:
        description_parts.append("\n### üõ†Ô∏è G·ª£i √Ω c√°c b∆∞·ªõc x·ª≠ l√Ω (Th·ªß c√¥ng)")
        for s in manual_steps:
            command = s.get('command') or s.get('target', 'N/A').replace('{{TARGET_HOST}}', host_alias)
            description_parts.append(f"- **{s.get('name')}**: `{command}`")

    labels = {"alertname": f"AIOps_{system_code}", "instance": instance, "severity": data.get('severity', 'warning'), "source": "AI_SRE", "host_alias": host_alias}
    if all_exec_results:
        labels["automation_attempted"] = "true"
        labels["automation_status"] = "SUCCESS" if all(res['result'].get('status') == 'SUCCESS' for res in all_exec_results) else "FAILED"
    
    return {"labels": labels, "annotations": {"summary": summary, "description": "\n".join(description_parts)}}

def send_to_alertmanager(payload, incident_id):
    """G·ª≠i c·∫£nh b√°o ƒë√£ ƒë·ªãnh d·∫°ng ƒë·∫øn Alertmanager."""
    if not ALERTMANAGER_URL:
        logger.info(f"[{incident_id}] ALERTMANAGER_URL not set. Skipping send.")
        return
    try:
        res = requests.post(ALERTMANAGER_URL, json=[payload], timeout=15)
        res.raise_for_status()
        logger.info(f"[{incident_id}] Successfully sent alert to Alertmanager.")
    except requests.RequestException as e:
        logger.error(f"[{incident_id}] Failed to send alert to Alertmanager: {e}")

# === K·∫æT TH√öC PH·∫¶N S·ª¨A L·ªñI NAMEERROR ===

def find_knowledge_candidates(host_identifier: str):
    """B∆∞·ªõc 1: Khoanh v√πng nhanh c√°c file .yml c√≥ hostname li√™n quan."""
    candidates = []
    if not os.path.exists(KNOWLEDGE_SOURCE_DIR):
        logger.error(f"Knowledge source directory not found: {KNOWLEDGE_SOURCE_DIR}")
        return candidates
    search_keys = {host_identifier}
    if '.' in host_identifier and not re.match(r'^\d{1,3}(?:\.\d{1,3}){3}$', host_identifier):
         search_keys.add(host_identifier.split('.')[0])
    for root, _, files in os.walk(KNOWLEDGE_SOURCE_DIR):
        for filename in files:
            if filename.endswith((".yml", ".yaml")):
                filepath = os.path.join(root, filename)
                doc = load_yaml_from_file(filepath)
                if not doc: continue
                hosts_in_doc = doc.get('metadata', {}).get('hosts', [])
                if not isinstance(hosts_in_doc, list): continue
                for host_info in hosts_in_doc:
                    if isinstance(host_info, dict) and any(key in {host_info.get('hostname'), host_info.get('ip')} for key in search_keys):
                        candidates.append({'doc': doc, 'host_info': host_info, 'filepath': filepath})
                        break
    return candidates

def get_best_candidate_by_llm_judge(alert_text: str, candidates: list, incident_id: str) -> dict | None:
    """B∆∞·ªõc 2: D√πng LLM ƒë·ªÉ th·∫©m ƒë·ªãnh v√† t√¨m ra ·ª©ng vi√™n duy nh·∫•t."""
    if not candidates:
        return None

    logger.info(f"[{incident_id}] Submitting {len(candidates)} candidates to LLM for judgment...")
    
    relevant_candidates = []
    for candidate in candidates:
        metadata = candidate['doc'].get('metadata', {})
        title = metadata.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        tags = ", ".join(metadata.get('tags', []))
        
        prompt = (
            f"B·ªëi c·∫£nh: M·ªôt log l·ªói ƒë√£ x·∫£y ra. "
            f"Log l·ªói: \"{alert_text}\".\n"
            f"T√†i li·ªáu h∆∞·ªõng d·∫´n c√≥ ti√™u ƒë·ªÅ: \"{title}\" v√† c√°c tags: \"{tags}\".\n"
            "C√¢u h·ªèi: D·ª±a v√†o log l·ªói, t√†i li·ªáu h∆∞·ªõng d·∫´n n√†y c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·ªÉ x·ª≠ l√Ω kh√¥ng? "
            "Ch·ªâ tr·∫£ l·ªùi 'C√ì' ho·∫∑c 'KH√îNG'."
        )
        
        messages = [{"role": "user", "content": prompt}]
        response = call_llm(messages, incident_id)
        
        logger.info(f"[{incident_id}] -> Judgment for '{os.path.basename(candidate['filepath'])}': LLM answered '{response}'")
        
        # === [S·ª¨A L·ªñI LOGIC] KI·ªÇM TRA C√ÇU TR·∫¢ L·ªúI NGHI√äM NG·∫∂T H∆†N ===
        # Ch·ªâ ch·∫•p nh·∫≠n n·∫øu c√¢u tr·∫£ l·ªùi b·∫Øt ƒë·∫ßu b·∫±ng "C√ì" (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
        first_word = response.split()[0].strip().lower() if response else ""
        if first_word == 'c√≥' or first_word == 'yes':
            relevant_candidates.append(candidate)

    if len(relevant_candidates) == 1:
        logger.warning(f"[{incident_id}] LLM JUDGMENT: Exactly one relevant document found. Proceeding with '{os.path.basename(relevant_candidates[0]['filepath'])}'.")
        return relevant_candidates[0]
    elif len(relevant_candidates) > 1:
        logger.error(f"[{incident_id}] AUTOMATION HALTED: Ambiguous situation. LLM found {len(relevant_candidates)} relevant documents. Escalating to human.")
        return None
    else: # len == 0
        logger.warning(f"[{incident_id}] AUTOMATION SKIPPED: LLM found no relevant documents.")
        return None

def process_incident(data: dict):
    """H√†m x·ª≠ l√Ω ch√≠nh cho m·ªói s·ª± c·ªë."""
    instance_identifier = data.get("instance", "unknown_instance").split(":")[0]
    incident_id = f"{datetime.now().strftime('%y%m%d_%H%M%S')}_{instance_identifier.replace('.', '_')}"
    logger.info(f"[{incident_id}] === New Incident Received: {data} ===")
    trigger_log = data.get('trigger_log', '')
    alert_content_to_analyze = f"{data.get('metric', '')} {trigger_log}"
    incident_context = f"S·ª± c·ªë ban ƒë·∫ßu: {json.dumps(data)}\n"
    if trigger_log: incident_context += f"Log g√¢y ra s·ª± c·ªë: {trigger_log}\n"
    related_logs = get_logs_for_instance(instance_identifier, around_time=datetime.fromtimestamp(data.get('timestamp', datetime.now().timestamp())), window_minutes=5)
    if related_logs: incident_context += f"C√°c log li√™n quan kh√°c:\n{related_logs}"
    
    initial_candidates = find_knowledge_candidates(instance_identifier)
    best_candidate = get_best_candidate_by_llm_judge(alert_content_to_analyze, initial_candidates, incident_id)
    
    all_execution_results, action_plan, knowledge_doc, knowledge_filepath = [], {}, None, None
    host_alias_for_ansible = instance_identifier
    analysis_summary = ""
    
    if best_candidate:
        knowledge_doc, matched_host_info, knowledge_filepath = best_candidate['doc'], best_candidate['host_info'], best_candidate['filepath']
        host_alias_for_ansible = matched_host_info.get('hostname', instance_identifier)
        action_plan = knowledge_doc.get('actionable_plan', {})
        remediation_playbooks = action_plan.get('remediation_playbooks', [])
        if isinstance(remediation_playbooks, list):
            for playbook in remediation_playbooks:
                is_allowed = playbook.get("allow_automation", False)
                has_auto_tag = "[AUTO]" in playbook.get("name", "")
                if is_allowed and has_auto_tag:
                    execution_result = dispatch_sre_action(playbook, matched_host_info.get('ip', instance_identifier), host_alias_for_ansible, incident_id)
                    all_execution_results.append({"name": playbook.get('name'), "result": execution_result})
                    if execution_result.get("status") == "SUCCESS" and any(k in playbook.get('name', '').lower() for k in STOP_ON_SUCCESS_KEYWORDS):
                        logger.warning(f"[{incident_id}] Remediation succeeded. Halting automation.")
                        break
                elif is_allowed and not has_auto_tag:
                    logger.warning(f"[{incident_id}] SKIPPING playbook '{playbook.get('name')}' because it is missing the '[AUTO]' tag.")
    
    automation_summary = "Kh√¥ng c√≥ h√†nh ƒë·ªông t·ª± ƒë·ªông n√†o ƒë∆∞·ª£c th·ª±c hi·ªán."
    if all_execution_results:
        automation_summary = "K·∫øt qu·∫£ c√°c h√†nh ƒë·ªông t·ª± ƒë·ªông ƒë√£ th·ª±c thi:\n"
        for res in all_execution_results:
            status = res['result'].get('status', 'N/A')
            output_line = res['result'].get('output', '').strip().splitlines()[-1] if res['result'].get('output') else "N/A"
            automation_summary += f"- Playbook '{res['name']}' c√≥ tr·∫°ng th√°i = {status}. K·∫øt qu·∫£: {output_line[:150]}\n"
    
    historical_context = memory.get_recent_history(instance_identifier)
    
    final_prompt = ""
    if best_candidate:
        final_prompt = (f"M√†y l√† m·ªôt chuy√™n gia SRE 20 nƒÉm kinh nghi·ªám. M·ªôt k·ªπ s∆∞ m·ªõi v√†o ngh·ªÅ ƒëang g·∫∑p log l·ªói n√†y: \"{trigger_log}\".\n"
                      f"ƒê√¢y l√† t√†i li·ªáu h∆∞·ªõng d·∫´n x·ª≠ l√Ω chu·∫©n c·ªßa c√¥ng ty (file {os.path.basename(knowledge_filepath)}):\n---\n{yaml.dump(knowledge_doc, allow_unicode=True)}\n---\n"
                      f"K·∫øt qu·∫£ c√°c h√†nh ƒë·ªông t·ª± ƒë·ªông (n·∫øu c√≥): {automation_summary}\n"
                      f"L·ªãch s·ª≠ c√°c s·ª± c·ªë t∆∞∆°ng t·ª± tr√™n m√°y n√†y: {historical_context}\n"
                      "D·ª±a v√†o t·∫•t c·∫£ th√¥ng tin tr√™n, h√£y vi·∫øt m·ªôt b·∫£n ph√¢n t√≠ch v√† h∆∞·ªõng d·∫´n chi ti·∫øt, t·ª´ng b∆∞·ªõc m·ªôt, cho k·ªπ s∆∞ ƒë√≥.")
    else:
        final_prompt = (f"M√†y l√† m·ªôt chuy√™n gia SRE. ƒê√¢y l√† m·ªôt l·ªói m·ªõi ch∆∞a c√≥ t√†i li·ªáu: \"{trigger_log}\".\n"
                      f"L·ªãch s·ª≠ c√°c s·ª± c·ªë tr√™n m√°y n√†y: {historical_context}\n"
                      "D·ª±a v√†o kinh nghi·ªám c·ªßa m√†y, h√£y ƒë∆∞a ra ch·∫©n ƒëo√°n v√† c√°c b∆∞·ªõc ki·ªÉm tra ban ƒë·∫ßu t·ªët nh·∫•t c√≥ th·ªÉ.")
    
    messages = [{"role": "user", "content": final_prompt}]
    analysis_summary = call_llm(messages, incident_id)
    
    short_summary = analysis_summary.splitlines()[0] if analysis_summary else "No AI analysis available."
    memory.add_event(incident_id=incident_id, entity_name=instance_identifier, summary=f"[{data.get('metric', 'log')}] {short_summary}")
    logger.info(f"[{incident_id}] Incident logged to context memory.")
    
    knowledge_source = os.path.basename(knowledge_filepath) if knowledge_filepath else "LLM General Analysis"
    title = knowledge_doc.get('metadata', {}).get('title', f"AI Analysis for {instance_identifier}") if knowledge_doc else f"AI Analysis for {instance_identifier}"
    system_code = knowledge_doc.get('metadata', {}).get('system_code', 'General') if knowledge_doc else "General"
    
    final_alert = format_final_alert(incident_id, data, instance_identifier, host_alias_for_ansible, analysis_summary, action_plan, all_execution_results, knowledge_source, title, system_code, trigger_log)
    send_to_alertmanager(final_alert, incident_id)
    
    try:
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        with open(os.path.join(OUTPUT_DIR, f"{incident_id}.md"), "w", encoding='utf-8') as f:
            f.write(f"# {final_alert['annotations']['summary']}\n\n{final_alert['annotations']['description']}")
    except Exception as e:
        logger.error(f"[{incident_id}] Could not write debug markdown file: {e}")
    
    return {"status": "processed", "incident_id": incident_id, "knowledge_source": knowledge_source}

@app.route("/alert", methods=["POST"])
def alert_endpoint():
    data = request.get_json()
    if not data or "instance" not in data:
        return jsonify({"status": "error", "message": "Invalid JSON or missing 'instance'"}), 400
    result = process_incident(data)
    return jsonify(result)

@app.route("/reload", methods=["POST"])
def reload_cache():
    try:
        cleared_items = load_yaml_from_file.cache_info().currsize
        load_yaml_from_file.cache_clear()
        logger.warning(f"YAML cache cleared successfully ({cleared_items} items).")
        return jsonify({"status": "success", "message": f"Cache cleared. {cleared_items} items removed."})
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

if __name__ == '__main__':
    # ƒê√£ x√°c nh·∫≠n v·ªõi file memory.py c·ªßa b·∫°n, kh√¥ng c·∫ßn g·ªçi init_db()
    logger.info(f"üöÄ AI SRE Alerting Service (v9.4 - FINAL) is ready.")
    serve(app, host='0.0.0.0', port=5001, threads=10)
